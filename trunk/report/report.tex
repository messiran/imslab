%TODO
% spell check, b.v. ispell op de uva
% zoeken op TODO punten

%% vi: set tabs top=2, set textwidth=80
\documentclass[a4paper,11pt]{article}

\usepackage{homework}
% \usepackage{graphicx, subfigure}
% \usepackage{verbatim}
% \usepackage{algorithm, algorithmic}
% \usepackage{amsmath, amsthm, amssymb}
\usepackage[english]{babel}

\title{Mean shift object tracking\\ Lab report for IMS\\ Tjerk Kostelijk, Bart Buter\\{mailtjerk, bjbuter}@gmail.com}

\date{December 22, 2009}

\begin{document}
\maketitle
\section{Introduction}
	Videos are a ubiquitous, in the UK it has been claimed that there is one observation camera per 14 citizens [CLAIM: "A CCTV camera for every 14 citizens."
	David Davis, resignation statement, 12 June 2008 ]. YouTube.com the largest video website in the USA with a marketshare of 99\% of all watched online videos serves more than 10 Billion movies a month[http://mashable.com/2009/09/28/august-comscore-data/]. 
	Furthermore, the amount of videos uploded to the site through mobile phones is growing exponentially[http://arstechnica.com/apple/news/2009/06/mobile-uploads-to-youtube-up-400-after-iphone-3gs-launch.ars].
	Ideally we would like to analyse video's to be able to extract their content, forinstance, to make them accessible for search or to alert the viewer of important developments. One of these analysis is tracking an object through multipe frames of video.

	Generally tracking can be done in two distinct manners; First, a model of the object to be tracked can be build, one can think of a wireframe model of the object together with texture or color information. 
	In every frame this model is searched for and the location that resembles the model best is the location of the object. A problem with this approach is that a model for every object needs to be build, usually can only be applied to small specialist domains. 
	The other approach does not build a full model but looks at the appearance of an object and looks searches for regions that appear the same, thus not a fully descriptive model is build. 
	The assumption is that small local features of the tracked model appear the same over multiple frames, one such feature is color.

	This paper will present and compare methods within the appearance based vision paradigm, namely the color based mean-shift tracker and brute-force tracker. 
	Since color will be the feature that will be tracked, different color models will be presented, applied and subsequent tracking results on diffent videos for both trackers will be discussed.
	The main focus of this paper will be to present and compare the mean-shift tracker and explain through experiments how the tracker behaves under varying video conditions. (such as occulusion and objects of varying scales), and what the influence of the used color model is.
	As said at the beginning ofthis section videos are pervasive, and their analysis should therefore ideally be done in realtime. The speed of the tracker will therefore also be discussed.

	This paper is organised as follows, after an introduction of the topic in section 1, the theory behind the used trackers and colorspaces will be discussed in section 2.
	Section 3 will be devoted to the implementation of the presented theories. The experimental setup, a description of the data, subsequent results and their discussion will be presented in section 4. 
	Finally, section 5 will present the main conclusions on color based mean shift tracking.

%TODO check
%Intro Bla
%Why we want it
%hat we are gonna do
%Paper Content
\section{Theory}	
	This section will explain the main theories used in developing the mean shift tracker. 
	\subsection{Colorspace}
		In the introduction we mentioned that we would track an object on its appearance and that the feature we would use would be color. In this section we will discribe three different color models and their relevance.

		\subsubsection{RGB}
		The color a camera dectects is dependend on the Spectral power distribution(SPD) of the light source, the reflectance factor of the object, which describes what part of the spectrum gets reflected and what gets absorbed, and the color matching functions of the camera sensors. 
		All perceivable colors can, based on trichrimacy theory[quote gevers], be modeled by the response of three sensors. 
		
		RGB is a standard colorspace used in computer vision, which desribes the detected colors as a tri-stimulus, (R,G,B). Where R corresponds the response of the Red sensor, which is most sensitive to Red, G to the response of the green sensor and B to the resonse of the Blue sensor.

		% TODO where should invariants go?, say some general about invariants and show table[Color Feature Detection, Geveres]
		However, this model has some problems for the domain of object tracking. it is not invariant to illumination intensity, which means that if the amount of light that falls on an object changes, th appearance changes, the human analog is the perception of shades. 
		Furthermore, RGB 
		

		\

		\subsubsection{normalised rgb}
		normalised RGB is a colormodel which is invariant to illumination intensity, orientation of the object surface and also viewpoint invariant. 
		normalised RGB is derived from RGB by deviding the reponse of each color channel by the intensity, thus creating an intensity invariant colorspace
		\subsubsection{Hue}
		The Hue is derived from a different color space, where a color is not described as the response of three RGB sensors but it is described as a Hue, which can be seen as a pure spectral color, saturation which describes how pure the color is, if it is fully satureated there is a pure spectral color, less saturated colors have more white mixed in. Last the intensity describes how bright the color is. 
		Hue is invariant to illumination and object and observer geometry and highlights. [quote invariant claim]
	\subsection{Histogram}



	\subsection{Brute force}
		First the tracked object is modeled by computing a color histogram, the
		target model.
		In the next frame a new color histogram is computed on every location,
		these are the candidate models.
		The target model is compared with all candidate models using a distance
		measure. The new location is determined by the location of the candidate
		histogram with the lowest distance.
		
		\paragraph{Semi Brute force}
		Using the fact that we work with 25 frames every second and objects are moving
		within a maximum speed, an assumption can be made that the object
		location should be close to the previous location. This optimalization
		ignores a lot of unnecessary computations.

	
	\subsection{Meanshift}
	Mean shift is an algorithm that finds the most probable target position of a
	tracked object in a frame.

	First a the tracked object is modeled by computing a color histogram.
	Likewise on the same location but in the next frame an other color histogram is
	computed.

	These two histograms are compared using a Bhattagarya distance measure. This
	results in a weight image where high values represent pixels that are
	simmilar.  For the intuition, when an object moved to the left, the weight
	values will be high on the left side. Furthermore the weights will be low on
	the right side because new information (different colors) entered the
	searchwindow. 

	This is used to weight an image which contain relative locations
	(where the origin is situated in the center of this image). This result in
	an image where each pixel represents a weighted shift. It is easy to see
	that the mean shift is the mean of this image.
	
	% TODO 
	%Above steps are done iteratively till a 

	% (2) is to be sure pixels that are farther away from the center are assigned a lower weight.

	% To prevent overshoot the mean shift vector is divided by 2 and added up to
	% the old location Y1.

	% The algorithm repeats on this new location until the mean shift vector is
	% smaller then a certain convergence threshold, for example 1 pixel.

\section{Implementation}

\section{Results} 
	\subsection{Experimental setup} 
	different colorspaces
	compare meanshift with brute force
	images etc.
	\subsection{Brute force} 
	\subsection{Meanshift} 
\subsection{Discussion} % discussion of results


\section{Conclusions} \label{sec:conc}
\section{Future Work} \label{sec:fut}
Observe how you can improve your design and them describe how you implemented this change or, for lack of time, describe how you would change your design. 

e.g. a different color space or the number of bins in the histogram
test the tracker also on a video of a domain other than soccer (or any other sport on a green field)

\section{References} 

% \begin{figure}[!ht]
% \centering
% \includegraphics[height=7cm]{img/fprate}
% \caption{The false positive rate per layer, averaged over the test set.}
% \label{fig:fprate}
% \end{figure}

% \renewcommand\bibname{References}
% \bibliography{references}
% \bibliographystyle{IEEEtran}
\end{document}
