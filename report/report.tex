%TODO
% spell check, b.v. ispell op de uva
% zoeken op TODO punten

%% vi: set tabs top=2, set textwidth=80

\documentclass[a4paper,11pt]{article}

\usepackage{homework}
% \usepackage{graphicx, subfigure}
% \usepackage{verbatim}
% \usepackage{algorithm, algorithmic}
% \usepackage{amsmath, amsthm, amssymb}
\usepackage[english]{babel}
\usepackage{url}
\usepackage{dsfont}
\usepackage{footmisc}

\title{Mean-shift Object Tracking\\ Lab Report for IMS}
\date{}
\begin{document}
\maketitle 

\section{Introduction}

	Videos are ubiquitous; in the United Kingdom it has been claimed that there is one observation camera per 14 citizens \footnote{

Rt Hon David Davis MP, June 12 2008, speech:"It is incumbent upon me to take a stand" 

\url{http://www.conservatives.com/News/Speeches/2008/06/David_Davis_It_is_incumbent_upon_me_to_take_a_stand.aspx}

}. YouTube.com, the largest video website in the USA with a marketshare of 40\% of all watched online videos serves more than 10 Billion movies a month\footnote{comScore Video Metrix, August 2009 

\url{http://comscore.com/Press_Events/Press_Releases/2009/9/Google_Sites_Surpasses_10_Billion_Video_Views_in_August}}.

	Furthermore, the amount of videos uploaded to the site through mobile phones is growing exponentially \footnote{The Official YouTube Blog, June 25 2009, blogpost:"Mobile Uploads to YouTube Increase Exponentially" 

\url{http://youtube-global.blogspot.com/2009/06/mobile-uploads-to-youtube-increase_5122.html}}. 

	From such facts, it is clear that the increasing signifance of video content, ranging from the practical to recreational, is undeniable. This plethora of video content makes automated video analysis desirable. Ideally we would like to analyse videos to be able to extract their content. This includes making them accessible for search or to alert the viewer of important developments. One of these analyses is tracking an object through multiple frames of video.



In this paper tracking is understood as identifying an object at a location in a video frame that is consistent with locations found in previous video frames, given an object description. An example of a inconsistent location would be that of an object previously on the left side between frames jumped to the right side of the videoframe. Generally, tracking can be done in two distinct manners; model based and appearance based. 

The former approach begins by building a substantial descriptive model of the object to be tracked. One can think of a wireframe model of the object together with texture or color information. In every frame this model is searched for, and the location that resembles the model best is the location of the relevant object.  A problem with this approach is that a substantial descriptive model has to be constructed for every conceivable object to be tracked.  Because of this tedious task, this approach can only be applied to small specialized domains.  



This paper, however, will analyse an instance of the latter, appearance based, approach. This approach does not build a full model for tracking, but rather it looks at the appearance of an object and searches for regions that appear the same.  This is thus not a substancial descriptive model; rather it is an appearance based model. The underlying assumption behind this way of tracking is that small local features of the tracked object appear the same over multiple frames. %One such feature is color.

The appearance based tracker discussed in this paper is the mean-shift tracker. 



Color is the feature that will be tracked, so different color models will be presented and applied. Following this, subsequent tracking results on different videos for both trackers will be discussed. Specifically, we want to explain through experiments how the mean-shift tracker behaves under varying video conditions (such as occlusion and objects of varying scales), and what the influence of the used color model is.

	As said at the beginning of this section, videos are pervasive, and their analysis should therefore ideally be done in realtime. The speed of the tracker will, therefore, also be discussed. 

	

	This paper is organised as follows:  after the introduction of the topic in section 1, the theory behind the used trackers and color models will be discussed in section 2.

	Section 3 will be devoted to the implementation of the presented theories. The experimental setup, a description of the data, subsequent results and their discussion will be presented in section 4. 

	Finally, section 5 will present the main conclusions on color based mean shift tracking.



\section{Theory}	

	This section will explain the main theories used in the mean shift tracker. The color of an object is dependent on what we can call the intrinsic color of the object and other factors such as lighting. Because the tracker will track an object on its color, it is important to choose a color model that is invarient to all factors except the instrinsic color of the object. Therefore in the following section, color models in general as well as some specific color models and their invarient properties will be introduced. Following this, histograms, which are probabilistic descriptions, or probability density functions (PDF), of color in image regions, will be discussed. We can compare PDFs using certain distance measures, which enables us to find color similarities between image regions. These concepts are integral to understanding the mean-shift tracker, which will be discussed afterwards. For an interesting comparison, the simpler brute force tracker willl be briefly discussed last.  



\subsection{Color models}

To clarify the content of this section it is important to take note of some characteristics of color. First, there is a difference between what we have called the intrinsic color of an object and the apparent color of that object. As an example, a white car looks white in normal daylight, whereas, when lit at night by the orange light of a sodium lamp, the white car will look orange. In this example, the intrinsic color of the car is white, while the appearent color is either white or orange depending on the lightsource.



Another notworthy detail is that an object rarely has uniform color, therefore, an object patch is defined as a sufficiently small region of an object such that it has uniform color properties. 



When a camera is aimed at an object patch, the color it detects is dependent on the following factors: 

\begin{itemize}

\item the Spectral Power Distribution (SPD) of the light source

\item the reflectance factor of a patch of the object

\item the color matching functions of the camera sensors 

\item the geometric arrangement of the lightsource(s), the object and the observer

\item the geometric properties of the object itself

\end{itemize}

The SPD describes for the whole visible light spectrum with what intensity every wavelength is being radiated. The reflectance factor describes how much each part of the spectrum gets reflected and how much gets absorbed by the object patch; this is what has been referred to as the intrinsic color. The color matching functions indicate the magnitude of the response of a camera sensor for every wavelength of incoming radiation. Last, the geometric arrangements and properties of the object also influence the color \cite{gevers_color}.



It is important to distinguish between the reflectance factor of a patch of an object, and the appearance of this patch's color to a camera. All factors described above influence the appearance of the patch, however the reflectance factor is the only one to stays constant. Therefore, we want to track an object based on its reflectance factor since this is also the only factor intrinsic to the object.  Hence, we want to have a color model which is ideally invariant to all factors that influence the appearance of the object except for the reflectance factor. In the following sections, various color models will be discussed in terms of their general properties and their invariance, in particular. 



\subsubsection{RGB and XYZ}

		

All percievable colors can, according to trichromacy theory \cite{gevers_color}, be modeled as the response of three sensors, namely: red, green, blue. The RGB color model resembles the human perception of colors in the sense that human color vision can also be said to be the result of a tri-stimulus.  RGB is an additive color model, which describes each color as a mixture of the three mentioned primary colors.



Some properties of the RGB color model is that it is a device dependent color model, which means that a given RGB color will be detected or reproduced differently on different devices. The three primary colors are not defined and therefore every device might use three different primary colors to detect or reproduce colors; this causes the device dependent property of this color model. Another property of RGB is that it is invariant to neither illumination intensity, highlights nor geometry \cite{gevers_invariant}. Since RGB is not invariant to any of the factors which influence the apparant color of an object patch, this color model seems ill-equipped for tracking. However, it is the standard model for representing colors and therefore worth mentioning.



The XYZ color model, just like RGB is also a tri-stimulus model, however it is defined  with 3 fictive primary colors X, Y and Z. The three colors are a linear combination of the RGB colors. However, the X,Y and Z colors are well defined, therefore, the XYZ colorspace is device independent. All invariant properties are the same for both RGB and XYZ.



\subsubsection{Normalised RGB and Normalised XYZ}



In the RGB color model, a change in lighting intensity would make the responses of all three color channels increase proportionally to the intensity change. Through normalisation, the lighting intensity can be fixed, which makes normalised RGB(rgb) invariant to illumination intensity. The geometric arrangements also influence all color channels proportionally and therefore influnece the color intensity of the perceived object patch. Invariance to illumination intensity, therefore, also creates invariance for geometric arrangement. rgb however is still not invariant to illumination color or highlights \cite{gevers_invariant}. 



The following equations describe the conversion from RGB to rgb, for each color channel respectively.  

Let $I$ denote the intensity:

\begin{eqnarray}
\label{eq:rgb}
I &=& R+G+B \\
r &=& \frac{R}{I},\; g = \frac{G}{I},\; b = \frac{B}{I}
\end{eqnarray}



Two things are worth concidering here, first $r,g,b$ are undefined for black when $I=0$, this can be remedied by recognising that white black and grey are the same except for intensity. Thus black in rgb space should be the same as white or grey which is $r=g=b$. Last the rgb colorspace is 3-dimensional; however, since all 3 channels are divided by the intensity, it must be that $r+g+b=1$. Therefore, rgb is overdetermined as a 3-dimensional space and can be described in 2 dimensions $r,g$, where $b$ can be deduced through $b=1-(r+g)$.



Similar to RGB, the device independent colorspace XYZ can be converted to a normalised XYZ(xyz).		

		

\subsubsection{HSV}

The hue, saturation, value model(HSV) does not represent color as a tri-stimulus, rather it uses three color properties to describe a color. Hue describes the dominant wavelength, saturation describes how dominant this wavelength is with respect to the whole of the visual spectrum, last value indicates the brightness of the color response, similar to intensity. The HSV-space can be thought of as a cone, where the central axis ranges between black at the bottom and white on top, the value indicates the distance along this central axis. The angle around the axis is described by the hue, while saturation indicates the distance from the central axis to the edge of the cone. 



When we defenestrate all but the hue, we are left with a 1-dimensional color space\footnote{Note the difference between a color space and a color model. The latter induces the former. For instance, the RGB 3-dimensional color space arises from the tri-stimulus color model, with 1 dimension for each color channel.}.  We can derive hue $H$ from RGB as follows:



\begin{equation}
\label{eq:hue}
H = arctan\left(\frac{\sqrt{3}\,(G-B)}{(R-G)+(R-B)}\right)
\end{equation} 



Describing color in this manner is invariant to illumination and geometrical considerations as well as highlights \cite{gevers_invariant}. The illumination intensity invariance can be understood in the following way: by removing the value we make our representation illumination independent, since value exactly describes this property.

		 

\subsection{Histogram}

In the previous section, various color models and their invariant properties were discussed. Once a color model has been chosen we are confronted with two problems; firstly, describing an object, and secondly, identifying an object in a videoframe given the object description and it's location in previous videoframes. These problems can be overcome with the use of histograms. This section describes histograms as a probability density function(PDF) over a color space. In further sections the histograms or PDFs that represent the object to be tracked will be called the target model.



We can describe an image patch containing an object by listing all the colors contained in the image patch as well as their relative frequencies. This yields a PDF over the color space, that describes the local color. This local color descriptor is scale invariant, since an object image patch of different dimension should still yield the same relative frequencies. The description is also shift and rotation invariant, since a rotated image of the object will again yield the same descriptor, and the exact coordinates of the image patch containing the object are irrelevant to the description of our object. The description, however, is not object orientation invariant since different sides of an object can have different colors. 



A PDF as described above creates a function over the whole color space, (for RGB, if every color channel is described by a byte, this leads to $2^{24}$ different colors). This will lead to a PDF which is too sparse, since most colors will not be in the image patch. Therefore, we collect similar colors in bins, and calculate the relative frequencies for each bin. In this paper the sizes of the bins are equal, thus every bin spans a equally sized portion of the color space. By accumulating colors in bins, we get a more coarse color description. The coarseness of the PDF is relevant since we want to use these PDFs to describe and compare local image descriptors. We want local image regions that look the same, to yield a local color descriptors that are similar.  Choosing the correct bin size will take care of this. However, the problem of picking the right bin size remains, too large bins would lead to a PDF which is too coarse, too small to a PDF that is too sparse. Therefore fine-tuning the number of bins is a delicate task of balancing between these two extremes. 



\subsubsection{Epanechnikov Weighting}

Histograms so far weight every color in an image region equally. However, it is reasonable to assume that the color near the center of the region is more important than the color near the borders. Therefore a weighting kernel is used to assign a larger importance to colors near the center than to colors near the border.



The mean shift tracker uses a kernel with Epanechnikov profile \cite{mean_shift_epan}, where "the profile of kernel K is defined as a function $k:[0,\infty)\rightarrow\mathbb{R}$ such that $K(x)=k(\|x\|^2)$."\cite{mean_shift}. The following equation describes the Epanechnikov profile and subsequent kernel: 



Let $x$ be a normalised location, such that the borders of our image region lie at distance 1. Then the Epanechnikov profile is



\begin{equation}
\label{eq:epanechnikov_profile}
k_E(x) = \left\{ \begin{array}{cl}
  1-x & \textrm{if } 0 \leq x \leq 1\\
  0 & \textrm{if }  x > 1 ,\end{array}\right.
\end{equation}

with a subsequent radially symmetric kernel in 2 dimensions, described as:

\begin{equation}
\label{eq:epanechnikov_kernel1}
K_E(x) = \left\{ \begin{array}{cl}
  \frac{2}{\pi} (1-\|x\|^2) & \textrm{if } \|x\| \leq 1 \\
  0 & \textrm{otherwise.} \end{array}\right.
\end{equation}

Let $x = (x_1,x_2)$ be a 2 dimensional vector indicating a normalised pixel location, such that after some manipulation we derive
\begin{equation}
\label{eq:epanechnikov_kernel2}
K_E(x) = \left\{ \begin{array}{cl}
  \frac{2}{\pi} (1-x_1^2 + x_2^2) & \textrm{if } \sqrt{x_1^2 + x_2^2} \leq 1 \\
  0 & \textrm{otherwise} \end{array}\right.
\end{equation}




The Epanechnikov kernel is used because the derivative of the profile equals $-1$, which makes the derivation and implementation of the mean shift tracker less troublesome than with another kernel though kernels such as Gaussians are feasable.



\subsubsection{Bhattacharyya Distance}

Multiple local color feature descriptors need to be compared to find the one that most closely resembles the description of the object we are tracking. Since the local feature descriptors are PDFs we need a similarity or distance measure between them. One such distance is the Bhattacharyya distance, which for discrete distributions is defined as:



\begin{equation}
\label{eq:bhattdistance}
d(p,q) = \sqrt{1-\sum_{u=1}^{m} \sqrt{p(u)\cdot q(u)}}.
\end{equation}

where $m$ denotes the number of bins, $p$ and $q$ are two discrete PDFs and $p(u)$ is the probability density for bin $u$ in PDF $p$. 



Intuitively it can be seen that when both $p$ and $q$ are similar, $\sqrt{p(u)\cdot q(u)}$ will be large for bins where both have accumulated a large probability mass. Therefore, $d(p,q)$ will be small. When two PDFs are dissimilar, none of the bins with a large probability mass will match up, therefore  $\sum_{u=1}^{m} \sqrt{p(u)\cdot q(u)}$ will be small, and $d(p,q)$ will be close to 1.



%TODO is local color descriptor introduced properly?



\subsection{Other tracking methods}

This section describes other methods of tracking, or object localising that have been explored for this project. Not all methods are used in the experimentation section, though all have been researched. A short description is therefore justified. Furthermore the intuition gained with brute force tracking will come to use in our discussion of the mean-shift tracker.



\subsubsection{Histogram back projection}

With a PDF representing the object to be tracked(target), we can take a picture and assign every pixel in that image the probability attached to the corresponding bin in the PDF. In this manner, pixels, which are assigned a high probability, are similar to many pixels in the target, since it falls in a bin with large probability. Likewise pixels with a low probability are largely dissimilar. The target can then be found in a region of the image that has high average probability. A problem with this approach is that it only looks at the dominant colors in the target and not to the similarity of the PDFs between regions. Furthermore, it requires an exhaustive assignment of probabilities to all pixels.


\subsubsection{Brute force}
%So we are talkign about this because it is interstingly silly. We want to say meanshift tracker is less computationally complex and therefore faster. 
This section will give a description of the brute force method, as a simple method of tracking useful as contrast to the mean-shift tracking method. As with histogram back projection, first a color histogram (the target model), is computed to model the object to be tracked. In a new frame, color histograms are computed at every location; these are the candidate models. The target model is compared with all candidate models using a distance measure. The new location is then determined by the location of the candidate histogram with the lowest distance.

One optimization of the brute force tracker we explored, exploits the knowledge that video consists of multiple frames per second and that objects and camera move within a reasonable range of speeds. Therefore, it follows that the object location should be close to the previous location, and thus only locations that can be reached within the range of reasonable speeds need to be considered. This makes the brute force approach somewhat more tractable. 

To understand brute force tracking intuitively, consider an image for which at every location the color has been replaced by the Bhattacharyya distance between the image region centered at that location and the target model. The resulting image can be seen as a height map, where the highest peaks represent the locations least similar to the target model, while the lowest are most similar. If one drops a marble onto this height map at the last known location of our target, the marble will roll to and stop at the locally lowest point. All points for which the marble will roll to a specific local minimum is the basement of attraction of that minimum. 

\subsection{Meanshift}

The thought behind the mean-shift algorithm is the following: Instead of computing all Bhattacharyya distances in a region close to the last known object position, one could take steps towards the local minimum of the Bhattacharyya distance. Therefore we need to create a shift vector which indicates in what direction and how big a step needs to be taken to obtain the lowest Bhattacharyya distance. If we are close to the local minimum, small steps are prudent, whereas further away larger steps are allowed. 



The shift vector is computed in the following manner, first weights for evey bin are calculated, in the following manner\footnote{For the derivation we refer to \cite{mean_shift}\label{fn:derivation_refer}}:



\begin{equation}
\label{eq:weights}
w(u) = \sqrt{\frac{q(u)}{p(u)}}
\end{equation} %TODO klopt dit?

where $w(u)$ is the weight, $q(u)$ is the target model, $p(u)$ is the candidate model all for bin $u$.



As an intuition when $p(u)$ and $q(u)$ are of equal magnitude, $w(u)$, will be close to 1, which can be considered a neutral weight, makes sense if you consider that the probability mass in both bins is similar thus no shift is needed. $\textrm{if }p(u) > q(u) \rightarrow w(u) < 1$.  $\textrm{if }p(u) < q(u) \rightarrow w(u) > 1$. Thus the weights are large for bins in  which have low probability mass in the candidate model, but height mass in the target model. Every location in the candidate region "pulls" at the center of the candiate region  weight of its coresponding bin times the distance to the center. Thus a location that lies far from the current center which has a large bin weight "pulls" hardest at the center, while either locations close to the center or with low weights pull less hard. The mean of this pull at the center, when normalised, is the shift vector. Which is calculated as follows, let $v_{shift}$ be the shift vector and $x$ be a normalised pixel location, with the center of the candidate region as the origin, $Pixels$ be all pixel locations in the candidate region, $c(x)$ be the color of a pixel at location $x$, $b(c(x))$ the bin corresponding to color $c(x)$ and $w(b(c(x)))$ the weight of the bin corresponding to the color of the pixel at location $x$. The following equation gives us the calculation of the mean shift vector\footref{fn:derivation_refer}:



\begin{equation}
\label{eq:shift vector}
v_{shift}=\frac{\sum_{x\in Pixels} x \ w(b(c(x)))}{\sum_{x\in Pixels} w(b(c(x)))}
\end{equation} %TODO klopt dit?

    

For the intuition, when an object moved to the left, the weight values will be high on the left side. Furthermore the weights will be low on the right side because new information (different colors) entered the search window. 

 

Bringing this all together in one algorithm, first the target model is obtained. Next, in every new frame the mean-shift algorithm starts at the last known position of the object, on this first candidate location a candidate model is created by computing the color histogram. Using both candidate and target models, $w(u)$ is computed for every bin, after which through the weighted shift or "pull" from every location the mean shift vector $v_{shift}$ can be obtained. Untill the shift vector is smaller than a pixel distance this is repeated. The resulting location is a local minimum and will be used as the location where the tracked object was found. This process is repeated for every image in the video\footnote{some checks and precautions are prudent, such as checking that the Bhattacharyya distance has indeed become less after a shift, or caution for large steps which "overshoot" the local minimum. }.



The mean-shift algorithm does not exhaustively compute all Bhattacharyya distances at locations in a region where the target might be found, but rather steps towards a local minimum of the Bhattacharya, thus making this algorithm more tractable. One main drawback is that the tracked object cannot move too much from the last known position, this would mean that the current position falls outside the basement of attraction of the target and will thus not be found. 



%problems tracking, 

% occulusion

% taking over the track by a different object

% changing lighting conditions

% moving out of the local search region\ basement of attraction.	



	

	% TODO 

	%Above steps are done iteratively till a 



	% (2) is to be sure pixels that are farther away from the center are assigned a lower weight.



	% To prevent overshoot the mean shift vector is divided by 2 and added up to

	% the old location Y1.



	% The algorithm repeats on this new location until the mean shift vector is

	% smaller then a certain convergence threshold, for example 1 pixel.



\section{Implementation}
The implementation of the mean-shift algorithm generally follows the implementation as discussed in \cite{mean_shift}. However, there are some noteworthy differences as well as some practical issues worth discussing in the following section. 

\subsection{Mean-shift differences}
The mean shift we implemented algorithm performs the following steps:
Given : 
\begin{itemize}
\item a target model $q$
\item the last target location $y_0$
\end{itemize}
For a new frame
\begin{enumerate}
\item \label{start} Assign the candidate location $y$ with $y_0$.
\item \label{loop} Calculate the candidate model $p$ at location $y$, keep the 1st candidate model as $p_0$
\item Calculate the weights for every bin, using $p$ and $q$.
\item Obtain the shift vector $v_{shift}$ using equation \ref{eq:shift vector}, using $w$ and normalised pixel locations $x$ in the candidate region.
\item rescale $v_{shift}$ to standard pixture sizes, "un-normalise"
\item If $v_{shift} < \frac{1}{2}$, thus rounded the shift is less than a pixel location\\
\begin{itemize}
\item if true, goto \ref{it:stop}
\item else, assign $y=y+v_{shift}$, goto \ref{loop}
\end{itemize}
\item \label{it:stop} check if Bhattacharyya distance between location $y$ and $y_0$ has indeed decreased with $d(p,q) < d(p_0,q)$ 
\begin{itemize}
\item if true, $y_0 = y$
\item else, $y_0 = y_0$, 
\end{itemize}
\item go to the next frame and goto \ref{start}
\end{enumerate}

A difference with the meanshift tracker as presented in \cite{mean_shift} is that we do not  abundantly check if the distance is indeed decreased at every step. Also, the "overshoot" prevention of taking half a shift step was not found to be required. Another difference, is the modified  Epanechnikov kernel other than the mean shift tracker in \cite{mean_shift} we do not use a circular kernel but a square kernel such that all four corners lay at distances $x=(\pm \sqrt{\frac{1}{2}},\pm \sqrt{\frac{1}{2}})$. The total weight of the kernel is then normalised to 1.
  
\subsection{Speed considerations}
During the development it was noticed that most of the time of the mean-shift tracker was spent loading and writing the tracked video frames. Therefore, all frames are pre-loaded in memory before the tracking starts. This has the benefit that the tracking is extremely fast, with the used datasets, tracking on a singlecore@3Ghz consumer grade PC, is more than 10x faster than real time, if the loading and saving of the images are not measured. The downside of this approach is that the computer needs to keep all frames stored in memory 





\section{Results} 

	\subsection{Experimental Setup} 
	In order to test the tracker, frames from a movie of a soccer match are
	used.  It is a trivial task for humans to track these soccer players. It is
	interesting to see how the mean shift algorithm compares.  Because the
	players have colored clothing and they move on a green background, the
	tracker, with particular color configurations, should work. Furthermore it
	provides an appropriate setup to compare different colorspaces .\\

	A single player is tracked. The player has variable speed because he is at
	different points accelerating or decelerating. A problem would occur if a
	player is moving so fast that he moves out of the region of interest in the
	next frame. In this instance, this is a problem because the players are
	relatively small and the camera had recorded 25 frames per second.
	
	In the first phase, we use a simple setup without occlusion. We downsampled
	the image 2x without smoothing or interpolation to test on data with bad
	quality.  We compare the performance of mean shift on different colorspaces
	and different binsizes.  We use 4 different colorspaces, RGB, XY, rg
	(normalised red and green), H (hue) and HSV (hue, saturation, intensity). We
	also use 4 different bin sizes. With bin size is meant the number of bins
	per color channel used to model the target/candidate histogram.

	In the second phase we test on occlusion. During tracking, a player is fully
	occluded by a player of another team. Moreover, to make it hard for the
	tracker, a smaller region is interest is used. 

	% TODO evt downsampling erin

=======

	binsize is per colorchannel

	\subsection{Brute force} 

	\subsection{Meanshift} 

	\begin{tabular}{l*{7}{r|}}
		\label{table:fase1}
		%\caption{Different colorspace, downsample and binsizes tested in fase 1}
		scene	& 	colorspace	& downsampled & 4 & 8 & 16 & 32\\

		\hline
		soccer 	& 	RGB	 		& 1x		  			  & + & + & +  &  -\\
		soccer 	& 	RGB	 		& 1x		  			  & + & + & +  &  -\\
		soccer 	& 	rg	 		& 1x 		  			  & + & + & +  &  -\\
		soccer 	& 	H	 		& 1x		 			  & + & + & +  &  -\\
		soccer 	& 	HS	 		& 1x		  			  & + & + & +  &  -\\
		soccer 	& 	HSV	 		& 1x		  			  & + & + & +  &  -\\
		soccer 	& 	RGB	 		& 2x		  			  & - & - & -  &  -\\
		soccer 	& 	rg	 		& 2x 		  			  & - & + & -  &  -\\
		soccer 	& 	H	 		& 2x		  			  & - & + & -  &  -\\
		soccer 	& 	HS	 		& 2x		  			  & - & + & -  &  -\\
		soccer 	& 	HSV	 		& 2x		  			  & - & - & -  &  -\\

	\end{tabular}	

	\begin{tabular}{l*{8}{r|}}
		scene	& 	colorspace	&  2 & 4 & 8 & 16 & 32\\
		\hline
		soccer 	& 	RGB	 		&  - & - & - & - & -\\
		soccer 	& 	XY	 		&  - & - & + & - & -\\
		soccer 	& 	rg	 		&  - & + & + & - & +\\
		soccer 	& 	H	 		&  - & - & - & + & +\\
		soccer 	& 	HSV	 		&  - & - & - & - & -\\ 
	\end{tabular}	

	\subsection{Brute force} 
\subsubsection{Conclusions}
As we look at table \ref{table:fase1} we can see different interesting results.
First of all the tracker performed very well on the images that where not
downsampled. Only for a very big binsize (32) the tracker failed. This can be
interpreted as problem of overfitting. The big binsize causes the model to
contain to much detail. If the model contains to much detail it cannot
generalise among different poses of the soccerplayer and therefor looses track.

The tracker performs generaly very bad on the downsampled image. This is a
logical consequence as the downsampling uses no interpolation or smoothing at
all causing the image to have a very bad quality. Only a few colorspaces
survived, rg, H and HS. These colorspaces performed well because they are
intensity invariant. Because the soccer player is moving he recieves different
illumination at different locations. This different illumination on different
locations holds for lots of tracking tasks, therefor intensity
invariant colorspaces are very usefull.

In fase two, where we introduce occlusion, we see again that only the intesity
invariant colorspaces keep track of the player. Furthermore we see that the Hue
colorspace only keeps track on very large bin sizes. This is because it uses
only one dimension. The less dimensions a colorspace needs the more bins it
needs to model a certain detail.

Furthermore rg performs better than XY, we think this is because the green
background of the soccerfield is modeled in more detail in the rg colorspace and
therefor rg has more discriminate power.


%TODO compare with other scene
\subsection{Discussion} % discussion of results
\section{Conclusions} \label{sec:conc}
\section{Future Work} \label{sec:fut}
Observe how you can improve your design and them describe how you implemented this change or, for lack of time, describe how you would change your design. 
e.g. a different color space or the number of bins in the histogram
test the tracker also on a video of a domain other than soccer (or any other sport on a green field)

Future work:
It would be nice to perform tests on different scales of downsampling with
different interpolation methods.


% variable tracking window size 
%TODO write something about kalman
%TODO would be better to have a ground truth and compair the tracking
%coordinates to the ground truth, no just a binary val (keeped track or not) is
%used

\section{References} 

% \begin{figure}[!ht]
% \centering
% \includegraphics[height=7cm]{img/fprate}
% \caption{The false positive rate per layer, averaged over the test set.}
% \label{fig:fprate}
% \end{figure}

\renewcommand\bibname{References}
\bibliography{references}
\bibliographystyle{IEEEtran}
\end{document}

